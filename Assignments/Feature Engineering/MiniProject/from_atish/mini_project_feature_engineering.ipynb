{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7b5bad-1db7-4133-8df6-e531bc6d0d01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'HR-Records_Final.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Read CSV File, read each Excel sheet seperately\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m df_app_data_dict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR-Records_Final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplication Data Dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m df_app_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR-Records_Final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mApplication data\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m df_candidate_data_dict \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHR-Records_Final.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,sheet_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCandidate Data Dict\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    481\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     io \u001b[38;5;241m=\u001b[39m ExcelFile(io, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, engine\u001b[38;5;241m=\u001b[39mengine)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     ext \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxls\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1652\u001b[0m     ext \u001b[38;5;241m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1653\u001b[0m         content_or_path\u001b[38;5;241m=\u001b[39mpath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options\n\u001b[1;32m   1654\u001b[0m     )\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExcel file format cannot be determined, you must specify \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man engine manually.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1659\u001b[0m         )\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/io/excel/_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m   1523\u001b[0m     content_or_path \u001b[38;5;241m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1525\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1526\u001b[0m     content_or_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m, storage_options\u001b[38;5;241m=\u001b[39mstorage_options, is_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1527\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[1;32m   1528\u001b[0m     stream \u001b[38;5;241m=\u001b[39m handle\u001b[38;5;241m.\u001b[39mhandle\n\u001b[1;32m   1529\u001b[0m     stream\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.11/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'HR-Records_Final.xlsx'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Read CSV File, read each Excel sheet seperately\n",
    "df_app_data_dict = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Application Data Dict\")\n",
    "df_app_data = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Application data\")\n",
    "df_candidate_data_dict = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Candidate Data Dict\")\n",
    "df_candidate_data = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Candidate Data\")\n",
    "df_interview_data_dict = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Interview Data Dict\")\n",
    "df_interview_data = pd.read_excel(\"HR-Records_Final.xlsx\",sheet_name=\"Interview Data \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e28ba2e-88df-42fd-9ece-c18a76aaca08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#the data in each Excel Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5c691c-5a34-4687-ae2f-7ee8ee0c28e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#App data \n",
    "df_app_data.head(3).append(df_app_data.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3694216d-5863-4180-b826-dba6f93143fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_candidate_data.head(3).append(df_candidate_data.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb116eb-3fbc-4ccf-902d-edd8922b5641",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_interview_data.head(3).append(df_interview_data.tail(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3825342e-334b-4b8e-bdca-e91600dda457",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Shapes\n",
    "\n",
    "print(\"Application Data Shape:\",df_app_data.shape)\n",
    "print(\"Candidate Data Shape:\", df_candidate_data.shape)\n",
    "print(\"Interview Data\",df_interview_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc3e23c-78c3-4f5c-84d2-83e78155df3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check columns \n",
    "print(df_app_data.columns)\n",
    "print(df_candidate_data.columns)\n",
    "print(df_interview_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e293078c-f789-4c23-8ed1-8d872ea5aa96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Combine all columns to find if there are duplicate by names\n",
    "all_columns = (df_app_data.columns.append(df_candidate_data.columns).append(df_interview_data.columns)).to_list()\n",
    "\n",
    "list.sort(all_columns)\n",
    "all_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4637cacb-f87e-4a34-9546-d05f8016431d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#By Looking at above column names, only 'Candidate ID' seems to be common across 3 Datasets.\n",
    "\n",
    "df_merged = df_app_data.copy()\n",
    "\n",
    "#merge candidate Data\n",
    "for col in df_candidate_data.columns:\n",
    "    df_merged[col] = df_candidate_data[col]\n",
    "\n",
    "#merge interview Data\n",
    "for col in df_interview_data.columns:\n",
    "    df_merged[col] = df_interview_data[col]\n",
    "    \n",
    "\n",
    "df_merged = df_app_data.merge(df_candidate_data,on=\"Candidate ID\")\n",
    "    \n",
    "df_merged = df_merged.merge(df_interview_data,on=\"Candidate ID\")\n",
    "df_merged.head(5)\n",
    "df_merged.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e66b5e-e09d-4b2b-a083-5412ff3b3476",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#from above merged DataFrame \"LOB\", \"Acceptance status\", \"Highest Educational Degree\", \"Candidate Source\", \"Interview types\", \"Interview Types2\" are catagory variables\n",
    "#Explore those columns\n",
    "print(\"Catagorical column Uniqueness\")\n",
    "print()\n",
    "print()\n",
    "print(\"LOB:\",df_merged[\"LOB\"].unique())\n",
    "print(\"Offered band:\",df_merged[\"Offered band\"].unique())\n",
    "print(\"Candidate relocation status:\",df_merged[\"Candidate relocation status\"].unique())\n",
    "print(\"Acceptance status:\", df_merged[\"Acceptance status\"].unique())\n",
    "print(\"Highest Educational Degree:\", df_merged[\"Highest Educational Degree\"].unique())\n",
    "print(\"Gender:\",df_merged[\"Gender\"].unique())\n",
    "print(\"Candidate Source:\",df_merged[\"Candidate source\"].unique())\n",
    "print(\"Interview types:\",df_merged[\"Interview types\"].unique())\n",
    "print(\"Interview Types2:\",df_merged[\"Interview Types2\"].unique())\n",
    "print(\"Joining location:\",df_merged[\"Joining location\"].unique())\n",
    "print(\"Location:\",df_merged[\"Location\"].unique())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba44fcd-68e4-42e5-99e2-c22b68fd47dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \"Highest Educational Degree\" is ordinal variable so map it with numerics\n",
    "education_degree_map = {\"Diploma\":0, \"Bachelors\":1, \"Master\":2}\n",
    "df_merged[\"Highest Educational Degree\"] = df_merged[\"Highest Educational Degree\"].map(education_degree_map)\n",
    "df_merged[\"Highest Educational Degree\"].head(5).append(df_merged[\"Highest Educational Degree\"].tail(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1ad355-9979-4263-9c22-b7f78401b0b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#check \"C6\" offered bank if its one of or many occurences. \n",
    "df_merged[df_merged[\"Offered band\"]==\"C6\"][\"Offered band\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a483a5b-7de2-4934-9cf3-9f91c7fbb654",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Check data quality \n",
    "#Check null/missing values\n",
    "df_merged\n",
    "#df_merged.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d5cd62-65f7-4d55-97bb-51cc296a144a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.pairplot(df_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f7c0fa-e775-4b4c-a2e7-4102ec6b1dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(df_merged.corr(numeric_only=True),annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7aab88a-e895-4b82-a622-3be7edb97888",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Encode the data\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "encoded_merged_data = df_merged.copy()\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "for c in [\"LOB\",\"Offered band\",\"Candidate relocation status\",\"Acceptance status\",\"Highest Educational Degree\",\"Gender\",\"Candidate source\",\"Interview types\",\"Interview Types2\",\"Joining location\",\"Current organization\",\"Location\",\"Acceptance status\"]:\n",
    "    encoded_merged_data[c] = encoder.fit_transform(encoded_merged_data[c])\n",
    "\n",
    "\n",
    "print(encoded_merged_data)\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(encoded_merged_data.corr(numeric_only=True),annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75069c09-992a-43fe-8049-f376f411dc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged[\"diff_current_offer_salary\"] = df_merged[\"Offered Salary\"] - df_merged[\"Current gross salary\"]\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b52b89b-9c28-4da7-9e1e-1972fcd0b423",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc403130-0084-4b38-afe7-6ed533489843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Analysis\n",
    "\n",
    "# \"Highest Educational Degree\", \"Interview rounds\", \"First interview duration\", \"Last interview duration\", \"Joning location\" are having weak relationship with other columns\n",
    "# \"Candidate Relocation Status\" - 90% of data is Yes , which suggest dataset is for relocated candidate.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Notice Period : 60   ----- 42 (Total) ---- 20 (joined) --- 20/42 = 0.47619047619047616\n",
    "\n",
    "# Notice Period : 45   ----- 52(Total) ------ 25 (joined)--- 25/52 =0.4807692307692308\n",
    "\n",
    "# Notice Period : 30   ----- 33(Total) ------ 17 (joined)--- 17/33 =0.5151515151515151\n",
    "\n",
    "# Notice Period : 15   ----- 27(Total) ------ 16 (joined)--- 16/27 =0.5925925925925926\n",
    "\n",
    "# Notice Period : 0   ----- 46(Total) ------ 24 (joined)--- 24/46 =0.5217391304347826\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "24/46"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8751e-0a1e-40d8-838f-412173e79918",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged.columns\n",
    "variables  = df_merged.columns.drop(['Candidate ID','Date of Application','Date of Interview','Date of Job Offer','Date of Offer extension','Date of Joining','DOB'])\n",
    "variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c67e9aa-8d56-461c-b740-0d002c1c2fa7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "variables  = [\"LOB\",\"Offered band\",\"Candidate relocation status\",\"Acceptance status\",\"Highest Educational Degree\",\"Gender\",\"Candidate source\",\"Interview types\",\"Interview Types2\",\"Acceptance status\"]\n",
    "visited_cols = []\n",
    "# Create cross-tabulations for each pair of categorical variables\n",
    "for outer_col in variables:\n",
    "    visited_cols.append(outer_col)\n",
    "    for inner_col in variables:\n",
    "        if visited_cols.count(inner_col) == 0:\n",
    "            cross_tab = pd.crosstab(df_merged[outer_col], df_merged[inner_col])\n",
    "            # Print the cross-tabulation table and discuss the results\n",
    "            print(f\"********* Cross-tabulation between {outer_col} and {inner_col}:\\n\")\n",
    "            print(cross_tab)\n",
    "            print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd15f2a2-d37c-4e8e-93ac-adf954c7135e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "variables  = [\"LOB\",\"Offered band\",\"Candidate relocation status\",\"Acceptance status\",\"Highest Educational Degree\",\"Gender\",\"Candidate source\",\"Interview types\",\"Interview Types2\",\"Acceptance status\"]\n",
    "\n",
    "# Create bar charts for each categorical variable\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df_merged, x=var, hue=\"Acceptance status\")\n",
    "    plt.title(f'{var} vs Acceptance status')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.xticks(rotation=80)\n",
    "    plt.legend(title='Income', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b1f4b-1c36-4031-b5ba-eb9797af03b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "merged_category_columns = ['First interview duration', 'Last interaction duration', 'Number of precious jobs','Total experience', 'LOB', 'Relevant experience', 'Year of graduation', 'Offered band', 'Notice period', 'Current organization', 'Highest Educational Degree','Joining location', 'Location', 'Interview rounds', 'Candidate relocation status', 'Gender', 'Candidate source', 'Interview types', 'Interview Types2']\n",
    "# Create bar charts for each categorical variable\n",
    "for var in merged_category_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=df_merged, x=var, hue='Acceptance status')\n",
    "    plt.title(f'{var} vs Acceptance status')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Acceptance status Count')\n",
    "    plt.xticks(rotation=80)\n",
    "    plt.legend(title='Acceptance status', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842642c9-5072-463d-befc-61bdea50b04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "numeric_vars = ['Offered Salary', 'Expected CTC', 'Current CTC', 'Current gross salary', 'Percentage hike offered (CTC)', 'Joining bonus amount', 'Number of precious jobs']\n",
    "for i in range(len(numeric_vars)):\n",
    "    for j in range (i+1, len(numeric_vars)):\n",
    "        var1 = numeric_vars[i]\n",
    "        var2 = numeric_vars[j]\n",
    "\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.scatterplot(data=df_merged, x=var1, y=var2, hue='Acceptance status', palette=['green', 'red'], alpha=0.5)\n",
    "        plt.title(f'Scatter Plot: {var1} vs {var2}')\n",
    "        plt.xlabel(var1)\n",
    "        plt.ylabel(var2)\n",
    "        plt.legend(title='Income', loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd936852-75ce-4459-80fc-c5d9f9fd0c5b",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0787e4f9-6cd4-488a-9ad4-91da38e4ea7d",
   "metadata": {},
   "source": [
    "\n",
    "#Quality of the data is good. There are no null, empty, Incosistent, noise data points. \n",
    "\n",
    "\n",
    "Acceptance status ----- This target variable\n",
    "\n",
    "\n",
    "# Following Columns are selected for further Data Exploration.\n",
    "\n",
    "+++ Notice period: There is probability that candidate will reject the offer if notice period >= 45days\n",
    "+++ Current organization: The candidate from some organizations has high probability to reject the offer.\n",
    "+++ Total experience: It looks like experience between 10 to 16 likely to reject the offer. \n",
    "+++ Gender: Although there is not strong relation with Acceptance status but as per the barchart there are chance probability of Female to join is bit higher than male. \n",
    "+++ Candidate source : There is probability that candidate will reject the offer if source from Agent instead of other 2 sources. \n",
    "+++ Location : Candidate from Bangluru, Mumbai and Mysore has high probability to reject the offer than other location.\n",
    "+++ Number of precious jobs: It has relationship with target variable. Candidate likely to reject between 3-6.\n",
    "+++ Interview rounds : The probability of offer rejection is high if there are <= 2 rounds\n",
    "+++ Interview Types2 : Candidates are likely to join if interview is Online. \n",
    "*** Interview types: Weak Dependency on Acceptance status. But keeping for further analysis.\n",
    "*** Offered band: Weak Dependency on Acceptance status. But keeping for further analysis.\n",
    "+++ Offered Salary - The probability of Rejection is high beween offered salary 2e6 to 3e6. \n",
    "+++ Last interaction duration: We see the pattern against 'Acceptance status'. The acceptance probability is high between 25 - 45.\n",
    "\n",
    "# We can consider following date type columns after data transformation.\n",
    "\n",
    "*** Date of Application\n",
    "*** Date of Job Offer\n",
    "*** Date of Offer extension\n",
    "*** Date of Joining\n",
    "*** Written Test Date\n",
    "*** Date of first Interview\n",
    "*** Date of last Interview\n",
    "\n",
    "\n",
    "\n",
    "#Following Columns are ignored for now for further Data exploration because of the reason mentioned for respective columns. \n",
    "\n",
    "Candidate ID ---- We should remove it as it is unique value per data point. \n",
    " \n",
    "--- Expected CTC : Very Strong relationship with 'Offered Salary'\n",
    "--- Current CTC : Very Strong relationship with 'Offered Salary'\n",
    "--- Current gross salary: Very Strong relationship with 'Offered Salary'\n",
    "--- Percentage hike (CTC) expected: It has strong relationship with 'Percentage hike offered (CTC)'\n",
    "--- Percentage hike offered (CTC) - It has weak relationship with target variable. No pattern observed in scattered plot.\n",
    "--- LOB - There is no strong relationship between Acceptance status. \n",
    "--- Joining location : Very Weak Relation with Acceptance status\n",
    "--- Candidate relocation status : Very Weak Relation with Acceptance status\n",
    "--- Highest Educational Degree: Highest Educational Degree\n",
    "--- Relevant experience: Very strong relation with Total Experience. We can ignore it.\n",
    "--- Year of graduation: It has strong relationship (inversely) with 'Total experience'. The chances offer rejection is high if candidate graduate between 2002 - 2010\n",
    "\n",
    "--- Joining bonus amount: Very Weak Relation with Acceptance status\n",
    "--- First interview duration: Weak relationship against Acceptance status.\n",
    "--- DOB - It is derived from based on the years of graduation and the degree\n",
    "--- Date of Interview: redundant data , same as  \"Date of last Interview\" feature data\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "1. Calculate Chi-squared, Information Gain, Gini Index, Fisher score to select Feature\n",
    "2. Evaluate significance of relocation map, i.e. candidate location and joining location, with acceptance status.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe98315-d3c2-4524-83fe-23df16211828",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Normalized Countplot for all identified columns for the exploration\n",
    "# Set the color coding for the visualizations to be plotted\n",
    "from matplotlib import cm\n",
    "cmap = cm.get_cmap('jet')\n",
    "\n",
    "#variables = [\"Notice period\",\"Current organization\",\"Total experience\",\"Gender\",\"Candidate source\",\"Location\",\"Number of precious jobs\",\"Interview rounds\",\"Interview Types2\",\"Interview types\",\"Offered Salary\",\"Last interaction duration\"]\n",
    "variables = [\"Acceptance status\",\"Notice period\",\"Total experience\",\"Gender\",\"Candidate source\",\"Location\",\"Number of precious jobs\",\"Interview rounds\",\"Interview Types2\",\"Interview types\",\"Offered Salary\",\"Last interaction duration\"]\n",
    "\n",
    "\n",
    "plt.figure(figsize=(12,8)) \n",
    "offset = 4;\n",
    "temp = 1\n",
    "for i,column in enumerate(variables[:offset], start=1):\n",
    "    plt.subplot(2,2,i) \n",
    "    df_merged[column].value_counts(normalize=True).plot.barh(figsize=(20,10), cmap=cmap)\n",
    "    plt.title(column)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "print(i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a052e376-f5cd-4720-9593-9d298e5024a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(15) \n",
    "variables = [\"Notice period\",\"Current organization\",\"Total experience\",\"Gender\",\"Candidate source\",\"Location\",\"Number of precious jobs\",\"Interview rounds\",\"Interview Types2\",\"Interview types\",\"Offered Salary\",\"Last interaction duration\"]\n",
    "\n",
    "# Create histograms with overlay of 'income'\n",
    "for var in variables:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.histplot(data=df_merged, x=var, hue='Acceptance status', kde=True, palette=['green', 'red'], alpha=0.5) #, hue_order=adult_data.income.unique())\n",
    "    plt.title(f'{var} vs Acceptance status')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    plt.legend(title='Acceptance status', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5230507e-ead4-46f7-becb-bd4b611da9bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged['offered and current salary diff'] = df_merged['Offered Salary'] - df_merged['Current gross salary']\n",
    "df_merged['days between first and last interview'] = (df_merged['Date of last Interview'] - df_merged['Date of first Interview']).dt.days\n",
    "df_merged['days between first interview and joining date'] = (df_merged['Date of Joining'] - df_merged['Date of first Interview']).dt.days\n",
    "df_merged['days between first interview and offer date'] = (df_merged['Date of Job Offer'] - df_merged['Date of first Interview']).dt.days\n",
    "df_merged['days between last interview and offer date'] = (df_merged['Date of Job Offer'] - df_merged['Date of last Interview']).dt.days\n",
    "df_merged['days between joining and extension date'] = (df_merged['Date of Joining'] - df_merged['Date of Offer extension']).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa02dce-d7b2-4228-85ce-17fb79215f61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged\n",
    "\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.heatmap(df_merged.corr(numeric_only=True),annot=True,cmap=\"coolwarm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ad3144-34c6-4fa2-9e47-ae011602242d",
   "metadata": {},
   "source": [
    "# Date Difference analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c782e26e",
   "metadata": {},
   "source": [
    "\n",
    "--- 'offered and current salary diff': Strong relationship with Offered Salary. \n",
    "--- 'days between first and last interview': Very strong relationship with 'days first interview and offer date'. Ignore it.\n",
    "--- 'days between last interview and offer date': No relation with any of the variable. It is always 10 days.\n",
    "--- 'days between first interview and offer date': It has strong relation with 'Interview Round'. We can remove it\n",
    "--- 'days between first interview and joining date': Strong relation with 'days joining and extension date' and Notice Period. We can remove it.\n",
    "--- 'days between joining and extension date': Strong relation with Notice Period. We can remove it.\n",
    "\n",
    "# we can ignore all date differences as it has strong relationship with \"Notice Period\", \"Interview Round\" which we alerady considered for further evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d05b31d-4d03-4c76-86c5-69f90610d250",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e0edde-240e-4f1a-86b0-07f3c7dcdad1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "datetime_col = ['Date of Application', 'Date of Interview', 'Date of Job Offer', 'Date of Offer extension','Date of Joining','Written Test Date','Date of first Interview','Date of last Interview']\n",
    "\n",
    "\n",
    "\n",
    "df_date_conversion  = df_merged.copy()\n",
    "for col in datetime_col:\n",
    "    month_col = col.replace('Date', 'Month')\n",
    "    year_col = col.replace('Date', 'Year')\n",
    "    df_date_conversion[year_col] = df_date_conversion[col].dt.year\n",
    "    df_date_conversion[month_col] = df_date_conversion[col].dt.month\n",
    "    #print(type(col))\n",
    "\n",
    "df_date_conversion[datetime_col].head(15)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ebeec5-c970-4728-94d9-a4dd5dece1c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#create the dataframe of required columns\n",
    "df_final = df_merged[[\"Notice period\",\"Current organization\",\"Total experience\",\"Gender\",\"Candidate source\",\"Location\",\"Number of precious jobs\",\"Interview rounds\",\"Interview Types2\",\"Interview types\",\"Offered band\",\"Offered Salary\",\"Last interaction duration\",\"Acceptance status\"]]\n",
    "df_final_base_copy = df_final\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743a6e1a-3074-4cfb-9495-098bf04ec1bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final['Total experience'].hist(bins=5, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b35731-1e90-4aac-b04d-26e712641a10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_final['Offered Salary'].hist(bins=20, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8df3a07-fae7-40d3-a145-6fd0a6156a04",
   "metadata": {},
   "source": [
    "# TODO revisit , if Trasnformation required for \"Total Experience\" as its ordinal Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0060805-cbe6-452b-a14e-95fef7e1b5f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert \"Total Experience\" into meaningful bins like \"Low\", \"Medium\", \"High\"\n",
    "\n",
    "# TODO revisit , if Trasnformation required for \"Total Experience\" as its ordinal Data\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "sorted_experiences=np.sort(df_final[\"Total experience\"]) \n",
    "\n",
    "\n",
    "#Width Binning\n",
    "ew_bins, intervals = pd.cut(sorted_experiences,bins=3,right=False,retbins=True)\n",
    "\n",
    "print(\"Width binning\",ew_bins)\n",
    "print(\"Width binning intervals\",intervals)\n",
    "\n",
    "#Frequency binning\n",
    "ef_bins, intervals = pd.qcut(sorted_experiences,q=3,retbins=True)\n",
    "\n",
    "print(\"frequence bins\",ef_bins)\n",
    "\n",
    "print(\"frequency intervals\",intervals)\n",
    "#df_final[\"Total Experience W bin\"]= ew_bins\n",
    "#df_final[\"Total Experience F bin\"]= ef_bins\n",
    "\n",
    "\n",
    "#encoder = LabelEncoder()\n",
    "#df_final[\"Total Experience W bin\"] = encoder.fit_transform(df_final[\"Total Experience W bin\"])\n",
    "#df_final[\"Total Experience F bin\"] = encoder.fit_transform(df_final[\"Total Experience F bin\"])\n",
    "#df_final[\"Acceptance status\"] = encoder.fit_transform(df_final[\"Acceptance status\"])\n",
    "\n",
    "\n",
    "\n",
    "#bin_proportions = df_final.groupby('Total Experience W bin')['Acceptance status'].mean()\n",
    "#print(bin_proportions)\n",
    "\n",
    "#bin_proportions = df_final.groupby('Total Experience F bin')['Acceptance status'].mean()\n",
    "#print(bin_proportions)\n",
    "\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.countplot(data=df_final, x=\"Total Experience F bin\", hue=\"Acceptance status\")\n",
    "#plt.title(f'Total Experience F bin vs Acceptance status')\n",
    "#plt.xlabel(\"Total Experience F bin\")\n",
    "#plt.ylabel('Count')\n",
    "#plt.xticks(rotation=80)\n",
    "#plt.legend(title='Acceptance status', loc='upper right')\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n",
    "\n",
    "\n",
    "#plt.figure(figsize=(10, 6))\n",
    "#sns.countplot(data=df_final, x=\"Total Experience W bin\", hue=\"Acceptance status\")\n",
    "#plt.title(f'Total Experience W bin vs Acceptance status')\n",
    "#plt.xlabel(\"Total Experience W bin\")\n",
    "#plt.ylabel('Count')\n",
    "#plt.xticks(rotation=80)\n",
    "#plt.legend(title='Acceptance status', loc='upper right')\n",
    "#plt.tight_layout()\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5455ae06-fd74-4401-8465-8d28433c63c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#catagories  'Location' to Zone to minimise possible values\n",
    "  \n",
    "  \n",
    "zone_map = {'Kolkata':'East', 'Pune':'West', 'Chennai':'South' ,'Hyderabad':'South', 'Bangaluru':'South', 'Mumbai':'West', 'Gurugram':'North','Mysore':'South', 'Delhi':'North', 'Noida':'North'}\n",
    "df_final[\"Location By Zone\"] = df_final[\"Location\"].map(zone_map)\n",
    "\n",
    "#Drop \"Location\" as its trasnformed to zones\n",
    "df_final = df_final.drop(['Location'],axis=1)\n",
    "\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253a520c-9bee-45d7-837b-51c1515161b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_final.columns)\n",
    "\n",
    "#for col in df_final.columns:\n",
    "df_final['Offered Salary'].hist(bins=20, grid=False)\n",
    "\n",
    "for var in df_final.columns:\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    #sns.histplot(data=df_merged, x=var, hue='Acceptance status', kde=True, palette=['green', 'red'], alpha=0.5) #, hue_order=adult_data.income.unique())\n",
    "    sns.histplot(data=df_final, x=var, kde=True, alpha=0.5) #, \n",
    "    plt.title(f'{var} distribution')\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel('Count')\n",
    "    #plt.legend(title='Acceptance status', loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d212b5-67bf-4a15-b87f-800f16747265",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convert \"Total Experience\" into meaningful bins like \"Low\", \"Medium\", \"High\"\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import datasets, linear_model, metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "sorted_offered_sal=np.sort(df_final[\"Offered Salary\"]) \n",
    "\n",
    "print(df_final[\"Offered Salary\"].describe())\n",
    "\n",
    "#Width Binning\n",
    "ew_bins, intervals = pd.cut(sorted_offered_sal,bins=4,right=False,retbins=True)\n",
    "print()\n",
    "print(\"Width binning\",ew_bins)\n",
    "print(\"Width binning intervals\",intervals)\n",
    "\n",
    "#Frequency binning ... later we analyzed frequency binning is not required so commenting\n",
    "#ef_bins, intervals = pd.qcut(sorted_offered_sal,q=4,retbins=True)\n",
    "\n",
    "df_final[\"Offered Salary bin\"]= ew_bins\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "df_final[\"Offered Salary bin\"] = encoder.fit_transform(df_final[\"Offered Salary bin\"])\n",
    "df_final[\"Acceptance status\"] = encoder.fit_transform(df_final[\"Acceptance status\"])\n",
    "\n",
    "\n",
    "\n",
    "bin_proportions = df_final.groupby('Offered Salary bin')['Acceptance status'].mean()\n",
    "print(bin_proportions)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.countplot(data=df_final, x=\"Offered Salary bin\", hue=\"Acceptance status\")\n",
    "plt.title(f'Offered Salary bin vs Acceptance status')\n",
    "plt.xlabel(\"Offered Salary bin\")\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=80)\n",
    "plt.legend(title='Acceptance status', loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df_final = df_final.drop(['Offered Salary'],axis=1)\n",
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac681866-8205-48d2-aaac-cc2ccc949e46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert all nominals to onehotEncoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Fit and transform the encoder on the 'Color' column\n",
    "oneHotEncoder  = OneHotEncoder()\n",
    "final_columns_to_be_one_hot_encoder = [\"Current organization\",\"Candidate source\",\"Location By Zone\",\"Interview Types2\",\"Interview types\",\"Offered band\"]\n",
    "#for col in final_columns_to_be_one_hot_encoder:\n",
    "#    df_final_encoded = oneHotEncoder.fit_transform(df_final[[col]])\n",
    "#    print(df_final_encoded)\n",
    "\n",
    "for col in final_columns_to_be_one_hot_encoder:    \n",
    "    # Fit and transform the encoder on the column\n",
    "    encoded_data = oneHotEncoder.fit_transform(df_final[[col]]) \n",
    "    # Convert the encoded data to a DataFrame\n",
    "    encoded_df = pd.DataFrame(encoded_data.toarray(), columns=oneHotEncoder.get_feature_names_out([col])) \n",
    "    # Concatenate the original DataFrame and the one-hot encoded DataFrame\n",
    "    df_final = pd.concat([df_final, encoded_df], axis=1)\n",
    "    # Drop the original column\n",
    "    df_final = df_final.drop([col], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Encode \"Gender\" column \n",
    "encoder = LabelEncoder()\n",
    "\n",
    "df_final['Gender'] = encoder.fit_transform(df_final['Gender'])\n",
    "df_final['Notice period'] = df_final['Notice period']/15\n",
    "df_final['Last interaction duration'] = df_final['Last interaction duration']/5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62be9bdb-f1da-4de2-89d4-1d763d841b8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(df_final['Notice period'].unique())\n",
    "print(df_final['Total experience'].unique())\n",
    "print(df_final['Number of precious jobs'].unique())\n",
    "print(df_final['Interview rounds'].unique())\n",
    "print(df_final['Last interaction duration'].unique())\n",
    "df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28d76020",
   "metadata": {},
   "source": [
    "# Normalize the final selected dataset based on visual analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3502128",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Perform Min-Max normalization on a given dataset.\n",
    "Parameters:\n",
    "data (numpy.ndarray): The data to be normalized.\n",
    "Returns:\n",
    "numpy.ndarray: The normalized data.\n",
    "\"\"\"\n",
    "def minmax_normalize(data):\n",
    "    min_val = np.min(data, axis=0)\n",
    "    max_val = np.max(data, axis=0)\n",
    "   \n",
    "    # Ensure the range is not zero to prevent division by zero\n",
    "    range_val = np.where(max_val - min_val == 0, 1, max_val - min_val)\n",
    "    \n",
    "    normalized_data = (data - min_val) / range_val\n",
    "    \n",
    "    return normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e960bc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "normailzed_final_data = minmax_normalize(df_final)\n",
    "normailzed_final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5114ce89-645e-493c-b908-22405aa252ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "df_final_temp = df_final\n",
    "#maintain dummy column prefix, useful when applying backward feature selection technique\n",
    "\n",
    "dummy_columns_prefix = [\"Current organization\",\"Candidate source\",\"Location By Zone\",\"Interview Types2\",\"Interview types\",\"Offered band\"]\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = normailzed_final_data.drop('Acceptance status', axis=1)\n",
    "y = normailzed_final_data['Acceptance status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logisticRegression = LogisticRegression()\n",
    "logisticRegression.fit(X_train, y_train)\n",
    "\n",
    "y_predict = logisticRegression.predict(X_test)\n",
    "print(\"Accuracy when All columns: \", accuracy_score(y_test,y_predict))\n",
    "\n",
    "for col in X_train.columns:\n",
    "    X_train = X_train.drop(col, axis=1)\n",
    "    X_test = X_test.drop(col, axis=1)\n",
    "    # Split data into training and testing sets\n",
    "    if len(X_train.columns) == 0:\n",
    "        break\n",
    "    logisticRegression = LogisticRegression()\n",
    "    #print(X_train.columns)\n",
    "    logisticRegression.fit(X_train, y_train)\n",
    "\n",
    "    y_predict = logisticRegression.predict(X_test)\n",
    "    print(\"Accuracy when drop column: \", col ,\" then accuracy is :\", accuracy_score(y_test,y_predict))\n",
    "\n",
    "    \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#print(\"Columns :::\", X_train.columns.drop(list(X_train.filter(regex=col_list))))\n",
    "\n",
    "#when we drop set of dummy columns\n",
    "for col_list in dummy_columns_prefix:\n",
    "    X_train_copy = X_train[X_train.columns.drop(list(X_train.filter(regex=col_list)))]\n",
    "    X_test_copy = X_test[X_test.columns.drop(list(X_test.filter(regex=col_list)))]\n",
    "    \n",
    "    logisticRegression = LogisticRegression()\n",
    "    logisticRegression.fit(X_train_copy, y_train)\n",
    "\n",
    "    y_predict = logisticRegression.predict(X_test_copy)\n",
    "    print(\"Accuracy when drop column list : \", col_list ,\" then accuracy is :\", accuracy_score(y_test,y_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79124bc9-a268-4f4a-9850-f62282924ad2",
   "metadata": {},
   "source": [
    "# Observation\n",
    "- The  acurracy score = 0.5 of the model is based on the manually selected features.\n",
    "- We tried to evaluate the score by eliminiting feature one by one but accuracy score remains same. \n",
    "- Now we will use Filter and Wrapper method on the Original DataSet where non of the data is transformed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0aa7b37",
   "metadata": {},
   "source": [
    "# # Try out Feature selection with Wrapper Method (Forward and Backward Selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606cfe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "df_final_forward = normailzed_final_data.copy()\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df_final_forward.drop('Acceptance status', axis=1)\n",
    "y = df_final_forward['Acceptance status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logisticRegression = LogisticRegression()\n",
    "selector = SequentialFeatureSelector(logisticRegression, n_features_to_select=5, direction='forward', scoring=\"balanced_accuracy\", tol=0.01, cv=5)\n",
    "X_train_selected_feature = selector.fit_transform(X_train,y_train)\n",
    "logisticRegression.fit(X_train_selected_feature, y_train)\n",
    "\n",
    "X_test_selected_feature = selector.transform(X_test)\n",
    "\n",
    "y_pred = logisticRegression.predict(X_test_selected_feature)\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9700e21d-5989-426b-b34f-4cbdcf801574",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "df_final_backward = normailzed_final_data.copy()\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = df_final_backward.drop('Acceptance status', axis=1)\n",
    "y = df_final_backward['Acceptance status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "logisticRegression = LogisticRegression()\n",
    "selector = SequentialFeatureSelector(logisticRegression, n_features_to_select=6, direction='backward', scoring=\"balanced_accuracy\", tol=0.01, cv=5)\n",
    "X_train_selected_feature = selector.fit_transform(X_train,y_train)\n",
    "logisticRegression.fit(X_train_selected_feature, y_train)\n",
    "\n",
    "X_test_selected_feature = selector.transform(X_test)\n",
    "\n",
    "y_pred = logisticRegression.predict(X_test_selected_feature)\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c8c9d7",
   "metadata": {},
   "source": [
    "# Now, lets try feature selection using Chi-Squared method with our normalized final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40e0c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "normailzed_final_data_chi2 = normailzed_final_data.copy()\n",
    "\n",
    "# Split data into features (X) and target (y)\n",
    "X = normailzed_final_data_chi2.drop('Acceptance status', axis=1)\n",
    "y = normailzed_final_data_chi2['Acceptance status']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Select the top k features based on chi-squared scores\n",
    "k = 5\n",
    "chi2_selector = SelectKBest(score_func=chi2, k=k)\n",
    "\n",
    "#1. Find the top 5 scored features using chi2\n",
    "# Fit the selector to your data\n",
    "chi2_selector.fit(X_train, y_train)  # X is your feature matrix, y is your target variable\n",
    "# Get the indices of the selected features\n",
    "selected_feature_indices = chi2_selector.get_support()\n",
    "\n",
    "# Use the indices to extract the names of the selected features\n",
    "selected_feature_names = [normailzed_final_data.columns[i] for i, selected in enumerate(selected_feature_indices) if selected]\n",
    "\n",
    "print(selected_feature_names)\n",
    "\n",
    "# 2. Find the score of features using chi2\n",
    "X_train_selected_features = chi2_selector.fit_transform(X_train, y_train)\n",
    "print(chi2_selector.scores_)\n",
    "\n",
    "# Fit the top selected features in LogisticRegression Model.\n",
    "X_train_chi2_selected_data = X_train[selected_feature_names]\n",
    "X_test_chi2_selected_data = X_test[selected_feature_names]\n",
    "\n",
    "logisticRegression = LogisticRegression()\n",
    "\n",
    "logisticRegression.fit(X_train_chi2_selected_data,y_train)\n",
    "\n",
    "y_pred = logisticRegression.predict(X_test_chi2_selected_data)\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "print(\"Balanced Accuracy:\", balanced_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c0e9b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
